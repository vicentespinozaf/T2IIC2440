{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZasAx5qUnJy_"
      },
      "source": [
        "# Tarea 2 PDM - Vicente Espinoza y Marcelo Vargas - Pregunta 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f85F5C7j5RiQ",
        "outputId": "ea73c46f-0587-444c-f6bd-d186d461b132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=059b9a91399cf796e9788b873c34a0b181850bd8370643d595069975cc5f8383\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vlxkf-AKWb-u"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Crear un SparkContext\n",
        "sc = SparkContext(\"local\", \"PageRank\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {
        "id": "XDHw1GGlLt5X"
      },
      "outputs": [],
      "source": [
        "# Función que para un nodo crea el mensaje a enviar como la división de su PageRank con la cantidad de sus vecinos\n",
        "def prepare_message(x):\n",
        "  node, current_rank, vecinos = x[0], x[1][0], x[1][1]\n",
        "  if type(vecinos) is not tuple:\n",
        "    if vecinos is None:\n",
        "      vecinos = (node,)\n",
        "    else:\n",
        "      vecinos = (vecinos, )\n",
        "  message = current_rank/len(vecinos)\n",
        "  return tuple((vecino, message) for vecino in vecinos)\n",
        "\n",
        "# Función que ejecuta la creación de los mensajes y los sumas de acuerdo al nodo que los recibió\n",
        "def send_messages(nodes, node_rank):\n",
        "  starting_node_sum = nodes.map(lambda x: (x, 0))\n",
        "  node_w_neighbors = edges.reduceByKey(lambda x, y: (x,) + (y,))\n",
        "  rank_w_neighbors = node_rank.leftOuterJoin(node_w_neighbors)\n",
        "  final_node_sum = rank_w_neighbors.flatMap(lambda x: prepare_message(x)).union(starting_node_sum).reduceByKey(lambda x, y: x + y)\n",
        "  return final_node_sum\n",
        "\n",
        "# Función que calcula el PageRank final de un nodo en base a los mensajes enviados por sus vecinos y el dumping_factor\n",
        "def update_rank(x, damping_factor, node_count):\n",
        "    page_rank = damping_factor * x[1] + ((1-damping_factor)/node_count)\n",
        "    return page_rank\n",
        "\n",
        "# Función que verifica si el RDD de Page Ranks converge en base a un epsilon\n",
        "def check_convergence(old_node_rank, new_node_rank):\n",
        "  eps = 0.001\n",
        "  rank_join = old_node_rank.join(new_node_rank)\n",
        "  rank_dif = rank_join.map(lambda x: abs(x[1][1] - x[1][0])).sum()\n",
        "  if 0.5 * rank_dif <= eps:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "# Función que en base a las funciones anteriores calcula el Page Rank para cada nodo\n",
        "def actualizar_rank(nodes, edges, damping_factor, iteraciones):\n",
        "  # Creamos un RDD con los nodos y sus Page Rank inicial\n",
        "  node_rank = nodes.map(lambda x: (x, 1/node_count))\n",
        "  # Contamos la cantidad total de nodos\n",
        "  node_count = nodes.count()\n",
        "  # Iteramos para actualizar los Page Ranks\n",
        "  for iteracion in range(iteraciones):\n",
        "    print(f\"Iteracion: {iteracion + 1}\")\n",
        "\n",
        "    # Enviamos los mensajes y creamos un RDD provisorio con los nuevos Page Ranks\n",
        "    node_messages = send_messages(nodes, node_rank)\n",
        "    new_node_rank = node_messages.map(lambda x: (x[0], update_rank(x, damping_factor, node_count)))\n",
        "\n",
        "    # Verificamos la convergencia\n",
        "    convergence_bool = check_convergence(node_rank, new_node_rank)\n",
        "    # Si hay convergencia se termina el algoritmo en la iteración actual\n",
        "    if convergence_bool:\n",
        "      print(f'Convergencia en la iteración {iteracion + 1}')\n",
        "      return node_rank\n",
        "    # Si no hay convergencia, se sigue y se termina el algoritmo hasta completar el número máximo de iteraciones\n",
        "    else:\n",
        "      node_rank = new_node_rank\n",
        "  print(f'No hubo convergencia en {iteracion + 1} iteraciones')\n",
        "  return node_rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nozFeu0Xkt5N",
        "outputId": "8b679d6f-88b8-4fde-9a50-919e010c502c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteracion: 1\n",
            "Iteracion: 2\n",
            "Iteracion: 3\n",
            "Iteracion: 4\n",
            "Iteracion: 5\n",
            "Iteracion: 6\n",
            "Iteracion: 7\n",
            "Iteracion: 8\n",
            "Iteracion: 9\n",
            "Iteracion: 10\n",
            "Iteracion: 11\n",
            "Iteracion: 12\n",
            "Iteracion: 13\n",
            "Iteracion: 14\n",
            "Iteracion: 15\n",
            "Iteracion: 16\n",
            "Iteracion: 17\n",
            "Iteracion: 18\n",
            "Iteracion: 19\n",
            "Convergencia en la iteración 19\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(1, 0.0), (2, 0.00048828125), (3, 0.0009765625), (4, 0.99853515625)]"
            ]
          },
          "execution_count": 317,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creamos un RDD con los nodos y otro con sus aristas\n",
        "nodes = sc.parallelize([1, 3, 2, 4])\n",
        "edges = sc.parallelize([(1, 2), (2, 3), (2, 4), (3, 2)])\n",
        "\n",
        "# Establecemos el damping factor y el número máximo de iteraciones.\n",
        "damping_factor, max_iteraciones = 1, 20\n",
        "\n",
        "# Llamamos la función\n",
        "actualizar_rank(nodes, edges, damping_factor, max_iteraciones).collect()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
